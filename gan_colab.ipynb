{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zu0iPVh9p2fS"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ],
   "id": "Zu0iPVh9p2fS"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TLs-pBGLtJY6"
   },
   "outputs": [],
   "source": [
    "!pip install torchdata\n"
   ],
   "id": "TLs-pBGLtJY6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lRiOO0HCr9ZS"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "id": "lRiOO0HCr9ZS"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3ffe8d60",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "device = torch.device('cuda')"
   ],
   "id": "3ffe8d60"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e33b7f74"
   },
   "outputs": [],
   "source": [
    "#descriminator\n",
    "\n",
    "bert = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ],
   "id": "e33b7f74"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "f2505aca"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "\n",
    "import torchdata.datapipes as dp\n",
    "import json\n",
    "from random import randrange\n",
    "\n",
    "def snli_train_all():\n",
    "    file = dp.iter.FileOpener(['./drive/MyDrive/snli_1.0/snli_1.0_train.jsonl']) \\\n",
    "        .readlines(decode=True, return_path=False, strip_newline=True) \\\n",
    "        .map(lambda line: json.loads(line.strip())) \\\n",
    "        .map(lambda line: (line['sentence1'], line['sentence2']))\n",
    "    return file"
   ],
   "id": "f2505aca"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "c7e6692a"
   },
   "outputs": [],
   "source": [
    "for param in bert.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "class BERT_Arch(nn.Module):\n",
    "    \n",
    "    def __init__(self, bert):\n",
    "        super(BERT_Arch, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "        self.fc2 = nn.Linear(512,2)\n",
    "        self.softmax = nn.LogSoftmax(dim = 1)\n",
    "    \n",
    "    def forward(self, sent_id, mask):\n",
    "        _, cls_hs = self.bert(sent_id, attention_mask = mask, return_dict = False)\n",
    "        x = self.fc1(cls_hs)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ],
   "id": "c7e6692a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6d15736d"
   },
   "outputs": [],
   "source": [
    "model = BERT_Arch(bert)\n",
    "\n",
    "model = model.to(device)\n",
    "from transformers import AdamW\n",
    "\n",
    "desc_optimizer = AdamW(model.parameters(), lr= 1e-3)"
   ],
   "id": "6d15736d"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "750927e0"
   },
   "outputs": [],
   "source": [
    "def desc_train(train_dataloader):\n",
    "    model.train()\n",
    "    cross_entropy = nn.CrossEntropyLoss()\n",
    "    for step, batch in tqdm(enumerate(train_dataloader), total = len(train_dataloader)):\n",
    "        batch = [r.to(device) for r in batch]\n",
    "        sent_id,mask,labels = batch\n",
    "\n",
    "        model.zero_grad()\n",
    "        preds = model(sent_id, mask)\n",
    "        loss = cross_entropy(preds, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        desc_grad = model.bert.embeddings.word_embeddings.weight.grad\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        desc_optimizer.step()\n",
    "        "
   ],
   "id": "750927e0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8f8f359c"
   },
   "outputs": [],
   "source": [
    "# GENERATOR\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm"
   ],
   "id": "8f8f359c"
  },
  {
   "cell_type": "code",
   "source": [
    "import torchdata.datapipes as dp\n",
    "import json\n",
    "\n",
    "\n",
    "class SnliLoader:\n",
    "    def snli_train_all(self):\n",
    "        file = dp.iter.FileOpener(['./drive/MyDrive/snli_1.0/snli_1.0_train.jsonl']) \\\n",
    "            .readlines(decode=True, return_path=False, strip_newline=True) \\\n",
    "            .map(lambda line: json.loads(line.strip())) \\\n",
    "            .map(lambda line: (line['sentence1'], line['sentence2']))\n",
    "        return file\n",
    "\n",
    "    def snli_valid_option(self, option):\n",
    "        file = dp.iter.FileOpener(['./drive/MyDrive/snli_1.0/snli_1.0_dev.jsonl']) \\\n",
    "            .readlines(decode=True, return_path=False, strip_newline=True) \\\n",
    "            .map(lambda line: json.loads(line.strip())) \\\n",
    "            .filter(lambda line: line['gold_label'] == option) \\\n",
    "            .map(lambda line: (line['sentence1'], line['sentence2']))\n",
    "        return file\n",
    "\n",
    "    def snli_train_option(self, option):\n",
    "        file = dp.iter.FileOpener(['./drive/MyDrive/snli_1.0/snli_1.0_train.jsonl']) \\\n",
    "            .readlines(decode=True, return_path=False, strip_newline=True) \\\n",
    "            .map(lambda line: json.loads(line.strip())) \\\n",
    "            .filter(lambda line: line['gold_label'] == option) \\\n",
    "            .map(lambda line: (line['sentence1'], line['sentence2']))\n",
    "        return file"
   ],
   "metadata": {
    "id": "iDdlJ_9wMx2V"
   },
   "id": "iDdlJ_9wMx2V",
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#vocab properties\n",
    "gen_tokenizer = 'spacy'\n",
    "\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n"
   ],
   "metadata": {
    "id": "Z2Y59OeEfOVJ"
   },
   "id": "Z2Y59OeEfOVJ",
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator, Vocab\n",
    "from typing import Iterable, List\n",
    "\n",
    "\n",
    "class Vocabulary(object):\n",
    "    token_transform = get_tokenizer(gen_tokenizer)\n",
    "    special_symbols = special_symbols\n",
    "    vocab_transform: Vocab\n",
    "\n",
    "    def __init__(self):\n",
    "        self.build()\n",
    "\n",
    "    def yield_tokens(self, data_iter: Iterable) -> List[str]:\n",
    "        for data_sample in data_iter:\n",
    "            yield self.token_transform(data_sample[0] + data_sample[1])\n",
    "\n",
    "    def build(self) -> Vocab:\n",
    "        snliLoader = SnliLoader()\n",
    "        train_iter = snliLoader.snli_train_all()\n",
    "        self.vocab_transform = build_vocab_from_iterator(self.yield_tokens(train_iter),\n",
    "                                                         min_freq=1,\n",
    "                                                         specials=self.special_symbols,\n",
    "                                                         special_first=True)\n",
    "        self.vocab_transform.set_default_index(UNK_IDX)\n",
    "        return self.vocab_transform\n",
    "\n",
    "    def get_vocab(self) -> Vocab:\n",
    "        return self.vocab_transform\n",
    "\n",
    "    def get_tokenizer(self):\n",
    "        return self.token_transform"
   ],
   "metadata": {
    "id": "vyjJBiatMfAS"
   },
   "id": "vyjJBiatMfAS",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#transformer properties\n",
    "\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "NUM_ENCODER_LAYERS = 4\n",
    "NUM_DECODER_LAYERS = 4"
   ],
   "metadata": {
    "id": "iEW8wutmgvTq"
   },
   "id": "iEW8wutmgvTq",
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "class WordEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(WordEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size: int, dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=EMB_SIZE,\n",
    "                                       nhead=NHEAD,\n",
    "                                       num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "                                       num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "                                       dim_feedforward=FFN_HID_DIM,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(EMB_SIZE, vocab_size)\n",
    "        self.src_tok_emb = WordEmbedding(vocab_size, EMB_SIZE)\n",
    "        self.tgt_tok_emb = WordEmbedding(vocab_size, EMB_SIZE)\n",
    "        self.positional_encoding = PositionalEncoding(EMB_SIZE, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "            self.tgt_tok_emb(tgt)), memory,\n",
    "            tgt_mask)"
   ],
   "metadata": {
    "id": "KfBU15pi_FbI"
   },
   "id": "KfBU15pi_FbI",
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#properties\n",
    "import torch\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "id": "9w4W2mI8ffZc"
   },
   "id": "9w4W2mI8ffZc",
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class MasksBuilder:\n",
    "\n",
    "    def create_mask(self, src, tgt):\n",
    "        src_seq_len = src.shape[0]\n",
    "        tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt_seq_len)\n",
    "        src_mask = torch.zeros((src_seq_len, src_seq_len), device=DEVICE).type(torch.bool)\n",
    "\n",
    "        src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "        tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "        return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask"
   ],
   "metadata": {
    "id": "8EdLT66hNVng"
   },
   "id": "8EdLT66hNVng",
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class SentenceGenerator:\n",
    "    def __init__(self, option, sentence_preprocessor, loss_fn, vocab, batch_size):\n",
    "        self.BATCH_SIZE = batch_size\n",
    "        self.sentence_preprocessor = sentence_preprocessor\n",
    "        self.option = option\n",
    "        self.vocab = vocab\n",
    "        self.model = Seq2SeqTransformer(len(vocab.vocab_transform))\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "        self.loss_fn = loss_fn\n",
    "        self.snli = SnliLoader()\n",
    "        self.mask_builder = MasksBuilder()\n",
    "\n",
    "        for p in self.model.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "        self.model = self.model.to(DEVICE)\n",
    "\n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        losses = 0\n",
    "        train_iteration = self.snli.snli_train_option(self.option)\n",
    "        train_dataloader = DataLoader(train_iteration, batch_size=self.BATCH_SIZE,\n",
    "                                      collate_fn=self.sentence_preprocessor.collate_fn)\n",
    "        counter = 0\n",
    "        for src, tgt in train_dataloader:\n",
    "            src = src.to(DEVICE)\n",
    "            tgt = tgt.to(DEVICE)\n",
    "            tgt_input = tgt[:-1, :]\n",
    "            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = self.mask_builder.create_mask(src, tgt_input)\n",
    "            logits = self.model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask,\n",
    "                                src_padding_mask)\n",
    "            self.optimizer.zero_grad()\n",
    "            tgt_out = tgt[1:, :]\n",
    "            loss = self.loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            losses += loss.item()\n",
    "            counter += 1\n",
    "        return losses / counter\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        losses = 0\n",
    "\n",
    "        val_iter = self.snli.snli_valid_option(self.option)\n",
    "        val_dataloader = DataLoader(val_iter, batch_size=self.BATCH_SIZE,\n",
    "                                    collate_fn=self.sentence_preprocessor.collate_fn)\n",
    "\n",
    "        counter = 0\n",
    "        for src, tgt in val_dataloader:\n",
    "            src = src.to(DEVICE)\n",
    "            tgt = tgt.to(DEVICE)\n",
    "\n",
    "            tgt_input = tgt[:-1, :]\n",
    "\n",
    "            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = self.mask_builder.create_mask(src, tgt_input)\n",
    "\n",
    "            logits = self.model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask,\n",
    "                                src_padding_mask)\n",
    "\n",
    "            tgt_out = tgt[1:, :]\n",
    "            loss = self.loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "            losses += loss.item()\n",
    "            counter += 1\n",
    "\n",
    "        return losses / counter\n",
    "\n",
    "    def greedy_decode(self, src, src_mask, max_len, start_symbol):\n",
    "        src = src.to(DEVICE)\n",
    "        src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "        memory = self.model.encode(src, src_mask)\n",
    "        ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "        for i in range(max_len - 1):\n",
    "            memory = memory.to(DEVICE)\n",
    "            tgt_mask = (self.mask_builder.generate_square_subsequent_mask(ys.size(0))\n",
    "                        .type(torch.bool)).to(DEVICE)\n",
    "            out = self.model.decode(ys, memory, tgt_mask)\n",
    "            out = out.transpose(0, 1)\n",
    "            prob = self.model.generator(out[:, -1])\n",
    "            _, next_word = torch.max(prob, dim=1)\n",
    "            next_word = next_word.item()\n",
    "\n",
    "            ys = torch.cat([ys,\n",
    "                            torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "            if next_word == EOS_IDX:\n",
    "                break\n",
    "        return ys\n",
    "\n",
    "    def translate(self, src_sentence: str):\n",
    "        self.model.eval()\n",
    "        src = self.sentence_preprocessor.text_transform(src_sentence).view(-1, 1)\n",
    "        num_tokens = src.shape[0]\n",
    "        src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "        tgt_tokens = self.greedy_decode(\n",
    "            src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "        return \" \".join(self.vocab.vocab_transform.lookup_tokens(list(tgt_tokens.cpu().numpy()))) \\\n",
    "            .replace(\"<bos>\", \"\") \\\n",
    "            .replace(\"<eos>\", \"\")\n",
    "\n",
    "    def backward_model(self, grads, sentence_batch):\n",
    "        src, tgt = self.sentence_preprocessor.collate_fn(sentence_batch)\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "        tgt_input = tgt[:-1, :]\n",
    "        new_grads = self.match_grads(grads, tgt_input)\n",
    "        new_grads = new_grads.to(DEVICE)\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = self.mask_builder.create_mask(src, tgt_input)\n",
    "        gen_ret = self.model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "        self.optimizer.zero_grad()\n",
    "        gen_ret.backward(new_grads)\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def match_grads(self, grads, token_batch):\n",
    "        pool_size = len(self.vocab.vocab_transform)\n",
    "        new_grads = torch.zeros(token_batch.size()[0], token_batch.size()[1], pool_size)\n",
    "        for batch_ind in range(grads.size()[0]):\n",
    "            for token_ind in range(grads.size()[1]):\n",
    "                if token_ind < token_batch.size()[0]:\n",
    "                    grad = grads[batch_ind, token_ind]\n",
    "                    new_grad = torch.zeros(pool_size)\n",
    "                    target_pos = token_batch[token_ind, batch_ind]\n",
    "                    new_grad[target_pos] = grad\n",
    "                    new_grads[token_ind, batch_ind] = new_grad\n",
    "        return new_grads\n"
   ],
   "metadata": {
    "id": "yMaDO6nUNhHf"
   },
   "id": "yMaDO6nUNhHf",
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class ModelLoader:\n",
    "    def __init__(self, saves_base_path=\"./.saved/spacy/test/\"):\n",
    "        self.saves_base_path = saves_base_path\n",
    "\n",
    "    def load_model(self, sentence_generator: SentenceGenerator):\n",
    "        checkpoint = torch.load(self.saves_base_path + sentence_generator.option + \".pt\")\n",
    "        sentence_generator.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        sentence_generator.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    def save_model(self, sentence_generator: SentenceGenerator):\n",
    "        torch.save({\n",
    "            'model_state_dict': sentence_generator.model.state_dict(),\n",
    "            'optimizer_state_dict': sentence_generator.optimizer.state_dict()\n",
    "        }, self.saves_base_path + sentence_generator.option + \".pt\")"
   ],
   "metadata": {
    "id": "7Z7VHKXONdZ7"
   },
   "id": "7Z7VHKXONdZ7",
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "class SentencePreprocessor:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab: Vocabulary = vocab\n",
    "        self.text_transform = self.sequential_transforms(self.vocab.get_tokenizer(),  \n",
    "                                                         self.vocab.get_vocab(),  \n",
    "                                                         self.tensor_transform)  \n",
    "\n",
    "    def sequential_transforms(self, *transforms):\n",
    "        def func(txt_input):\n",
    "            for transform in transforms:\n",
    "                txt_input = transform(txt_input)\n",
    "            return txt_input\n",
    "\n",
    "        return func\n",
    "\n",
    "    def tensor_transform(self, token_ids: List[int]):\n",
    "        return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                          torch.tensor(token_ids),\n",
    "                          torch.tensor([EOS_IDX])))\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "\n",
    "        src_batch, tgt_batch = [], []\n",
    "        for src_sample, tgt_sample in batch:\n",
    "            src_batch.append(self.text_transform(src_sample.rstrip(\"\\n\")))\n",
    "            tgt_batch.append(self.text_transform(tgt_sample.rstrip(\"\\n\")))\n",
    "\n",
    "        src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "        tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "        return src_batch, tgt_batch"
   ],
   "metadata": {
    "id": "AFq1vN6ukV_b"
   },
   "id": "AFq1vN6ukV_b",
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "\n",
    "class Generator:\n",
    "\n",
    "    def __init__(self, batch_size):\n",
    "        torch.manual_seed(0)\n",
    "        self.vt = Vocabulary()\n",
    "        self.VOCAB_SIZE = len(self.vt.build())\n",
    "        self.BATCH_SIZE = batch_size\n",
    "        self.NUM_EPOCHS = 1\n",
    "        self.sentence_preprocessor = SentencePreprocessor(self.vt)\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "        self.ENTAILMENT = 'entailment'\n",
    "        self.CONTRADICTION = 'contradiction'\n",
    "        self.model_loader = ModelLoader()\n",
    "        self.subgenerators = {\n",
    "            self.ENTAILMENT: SentenceGenerator(self.ENTAILMENT, self.sentence_preprocessor, self.loss_fn, self.vt, self.BATCH_SIZE),\n",
    "            self.CONTRADICTION: SentenceGenerator(self.CONTRADICTION, self.sentence_preprocessor, self.loss_fn, self.vt, self.BATCH_SIZE)\n",
    "        }\n",
    "\n",
    "    def train_model(self):\n",
    "        for epoch in range(1, self.NUM_EPOCHS + 1):\n",
    "            for option in [self.ENTAILMENT, self.CONTRADICTION]:\n",
    "                start_time = timer()\n",
    "                train_loss = self.subgenerators[option].train_epoch()\n",
    "                end_time = timer()\n",
    "                val_loss = self.subgenerators[option].evaluate()\n",
    "                print(f\"Epoch: {epoch}, GeneratorPart: {option}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\")\n",
    "\n",
    "    def evaluate(self):\n",
    "        for option in [self.ENTAILMENT, self.CONTRADICTION]:\n",
    "            val_loss = self.subgenerators[option].evaluate()\n",
    "            print(f\"GeneratorPart: {option}, Val loss: {val_loss:.3f}\")\n",
    "\n",
    "    def load_state(self):\n",
    "        for option in [self.ENTAILMENT, self.CONTRADICTION]:\n",
    "            self.model_loader.load_model(self.subgenerators[option])\n",
    "\n",
    "    def save_state(self):\n",
    "        for option in [self.ENTAILMENT, self.CONTRADICTION]:\n",
    "            self.model_loader.save_model(self.subgenerators[option])\n",
    "\n",
    "    def generate(self, src: str):\n",
    "        return self.subgenerators[self.ENTAILMENT].translate(src), self.subgenerators[self.CONTRADICTION].translate(src)\n"
   ],
   "metadata": {
    "id": "UYbO0Ntaf7-H"
   },
   "id": "UYbO0Ntaf7-H",
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#GAN\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "generator = Generator(BATCH_SIZE)\n",
    "\n",
    "pretrained_base_path = './drive/MyDrive/diplom-collab/models/4_l/'\n",
    "\n",
    "def load_model():\n",
    "  for option in [generator.ENTAILMENT, generator.CONTRADICTION]:\n",
    "    checkpoint = torch.load(pretrained_base_path + option + \".pt\")\n",
    "    generator.subgenerators[option].model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    generator.subgenerators[option].optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "  \n",
    "load_model()"
   ],
   "metadata": {
    "id": "_y2Y2cAhhEnI"
   },
   "id": "_y2Y2cAhhEnI",
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def tokens_grads_from_emb(embedding_weight, tokens):\n",
    "    pool_size = embedding_weight.size()[0]\n",
    "    grad_x = torch.zeros(pool_size)\n",
    "    for i in range(pool_size):\n",
    "        grad_x[i] = torch.matmul(embedding_weight[i], embedding_weight.grad[i])\n",
    "    tokens_grad = tokens.clone()\n",
    "    for token_ind in range(tokens_grad.size()[0]):\n",
    "        for batch_ind in range(tokens_grad.size()[1]):\n",
    "            if tokens_grad[token_ind][batch_ind] != 0:\n",
    "                ind = tokens[token_ind][batch_ind]\n",
    "                tokens_grad[token_ind][batch_ind] = grad_x[ind]\n",
    "    return tokens_grad"
   ],
   "metadata": {
    "id": "ogMq8zlqhqH4"
   },
   "id": "ogMq8zlqhqH4",
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def create_dataloader_for_desc(data, label):\n",
    "  df = pd.DataFrame(data={'text': data, 'target': [label for _ in range(len(data))]})\n",
    "\n",
    "  train_text = df['text'].astype('str')\n",
    "  train_labels = df['target']\n",
    "\n",
    "  tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.values,\n",
    "    max_length = 50,\n",
    "    padding = 'max_length',\n",
    "    truncation = True\n",
    "  )\n",
    "\n",
    "  train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "  train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "  train_y = torch.tensor(data=train_labels.values)\n",
    "\n",
    "  train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "  train_sampler = RandomSampler(train_data)\n",
    "  train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = BATCH_SIZE)\n",
    "  return train_dataloader\n"
   ],
   "metadata": {
    "id": "vQO1Gs4pswuF"
   },
   "id": "vQO1Gs4pswuF",
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#total_len = 550_152\n",
    "LIMIT = 10_000\n",
    "\n",
    "def prepare_train_data(epoch):\n",
    "  iter = snli_train_all().enumerate().filter(lambda pair: pair[0] >= LIMIT*(epoch-1) and pair[0] < LIMIT*epoch).map(lambda pair: pair[1])\n",
    "\n",
    "  data_desc_real = []\n",
    "  data_desc_fake = []\n",
    "\n",
    "  data_gen_ent = []\n",
    "  data_gen_contr = []\n",
    "\n",
    "  for src, tgt in tqdm(iter, total = LIMIT):\n",
    "    data_desc_real.append(src)\n",
    "    data_desc_real.append(tgt)\n",
    "    ent, cont = generator.generate(src)\n",
    "    data_desc_fake.append(ent)\n",
    "    data_desc_fake.append(cont)\n",
    "    data_gen_ent.append((src, ent))\n",
    "    data_gen_contr.append((src, cont))  \n",
    "\n",
    "  data_gen = {generator.ENTAILMENT: data_gen_ent, generator.CONTRADICTION: data_gen_contr}\n",
    "  return data_desc_real, data_desc_fake, data_gen"
   ],
   "metadata": {
    "id": "FSr7dPDNSCxa"
   },
   "id": "FSr7dPDNSCxa",
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train_desc(data_desc_real, data_desc_fake):\n",
    "  real_desc_dataloader = create_dataloader_for_desc(data_desc_real, 1)\n",
    "  fake_desc_dataloader = create_dataloader_for_desc(data_desc_fake, 0)\n",
    "  desc_train(real_desc_dataloader)\n",
    "  desc_train(fake_desc_dataloader)"
   ],
   "metadata": {
    "id": "OuajOZWY05gt"
   },
   "id": "OuajOZWY05gt",
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def create_dataloader_for_gen(data):\n",
    "  df = pd.DataFrame(data={'text': [pair[0] for pair in data], 'target': [0 for _ in range(len(data))]})\n",
    "\n",
    "  train_text = df['text'].astype('str')\n",
    "  train_labels = df['target']\n",
    "\n",
    "  tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.values,\n",
    "    max_length = 50,\n",
    "    padding = 'max_length',\n",
    "    truncation = True\n",
    "  )\n",
    "\n",
    "  train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "  train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "  train_y = torch.tensor(data=train_labels.values)\n",
    "\n",
    "  train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "  train_sampler = RandomSampler(train_data)\n",
    "  train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = BATCH_SIZE)\n",
    "  return train_dataloader"
   ],
   "metadata": {
    "id": "WZk-quo4Vx0l"
   },
   "id": "WZk-quo4Vx0l",
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train_gen(data):\n",
    "    model.train()\n",
    "    cross_entropy = nn.CrossEntropyLoss()\n",
    "    for option in [generator.ENTAILMENT, generator.CONTRADICTION]:\n",
    "        generator.subgenerators[option].model.train()\n",
    "        train_dataloader = create_dataloader_for_gen(data[option])\n",
    "        for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "            batch = [r.to(DEVICE) for r in batch]\n",
    "            sent_id, mask, labels = batch\n",
    "            model.zero_grad()\n",
    "            preds = model(sent_id, mask)\n",
    "            loss = cross_entropy(preds, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            embedding = model.bert.embeddings.word_embeddings.weight\n",
    "\n",
    "            desc_grads = tokens_grads_from_emb(embedding, sent_id)\n",
    "            sentences_batch = data[option][step * BATCH_SIZE:(step + 1) * BATCH_SIZE]\n",
    "            generator.subgenerators[option].backward_model(desc_grads, sentences_batch)"
   ],
   "metadata": {
    "id": "isX1lWpjh8if"
   },
   "id": "isX1lWpjh8if",
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# paired models\n",
    "NUM_EPOCHS = 1\n",
    "for epoch in range(3, NUM_EPOCHS+3):\n",
    "    data_desc_real, data_desc_fake, data_gen = prepare_train_data(epoch)\n",
    "    train_desc(data_desc_real, data_desc_fake) \n",
    "    train_gen(data_gen)\n",
    "\n",
    "generator.evaluate()"
   ],
   "metadata": {
    "id": "R3Gpy3-Tr9D0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "R3Gpy3-Tr9D0",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "descriminator_and_generator_split_classes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}